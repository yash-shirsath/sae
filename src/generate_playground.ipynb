{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96975bc9",
   "metadata": {},
   "source": [
    "# Playground for Comparing Pipeline Outputs and Investigating Pipeline Memory Leaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312543bf-7f6b-412b-b3ca-121cafc73df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, DDIMScheduler\n",
    "from diffusers.utils import make_image_grid\n",
    "from IPython.display import Image, display\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a171604-def9-4046-a205-d6129518da39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe1 = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", safety_checker=None)\n",
    "device = \"cuda\"\n",
    "pipe1.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fbe936",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipe2 = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", safety_checker=None)\n",
    "pipe2.to(device)\n",
    "pipe2.scheduler = DDIMScheduler.from_config(pipe2.scheduler.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac588492",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(pipe1.scheduler))\n",
    "print(type(pipe2.scheduler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe0f77b-b5b0-4db7-8699-0bba60d362ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid(prompts: list[str], cols: int, pipeline1: StableDiffusionPipeline, pipeline2: StableDiffusionPipeline | None = None, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Generate a grid of images from multiple prompts using batch processing.\n",
    "    If pipeline2 is provided, results are displayed side by side for comparison.\n",
    "    \n",
    "    Args:\n",
    "        prompts: List of prompts to generate images from\n",
    "        cols: Number of images to generate per prompt\n",
    "        pipeline1: First StableDiffusionPipeline to use for generation\n",
    "        pipeline2: Optional second StableDiffusionPipeline to use for generation\n",
    "        seed: Random seed for deterministic generation\n",
    "    \"\"\"\n",
    "    rows = len(prompts)\n",
    "    \n",
    "    \n",
    "    print(f\"Generating {cols} images for each of {rows} prompts with pipeline1\")\n",
    "    \n",
    "    # Generate images with first pipeline\n",
    "    with torch.no_grad():\n",
    "        generator1 = torch.Generator(device=pipeline1.device).manual_seed(seed)\n",
    "        results1 = pipeline1(prompts, num_images_per_prompt=cols, generator=generator1)\n",
    "        images1 = results1.images\n",
    "    \n",
    "    if pipeline2:\n",
    "        print(f\"Generating {cols} images for each of {rows} prompts with both pipelines\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generator2 = torch.Generator(device=pipeline2.device).manual_seed(seed)\n",
    "            results2 = pipeline2(prompts, num_images_per_prompt=cols, generator=generator2)\n",
    "            images2 = results2.images\n",
    "        \n",
    "        # Combine images from both pipelines\n",
    "        combined_images = []\n",
    "        for i in range(0, len(images1), cols):\n",
    "            combined_images.extend(images1[i:i+cols])\n",
    "            combined_images.extend(images2[i:i+cols])\n",
    "        \n",
    "        # Create and display the grid with double the number of columns\n",
    "        grid = make_image_grid(combined_images, rows=rows, cols=cols*2)\n",
    "    else:\n",
    "        # Create and display the grid with images from pipeline1 only\n",
    "        grid = make_image_grid(images1, rows=rows, cols=cols)\n",
    "        \n",
    "    display(grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36c408b4-e397-4110-94eb-66642384cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a cat\", \"a dog\", \"a horse\", \"a bird\", \"trees\", \"a dog sunbathing\", \"a dog front profile\", \"a dog side profile\"]\n",
    "run_grid(prompts, 3, pipe1, pipe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbbc4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "\"A bear as a painter working on a landscape\",\n",
    "\"A bear as a soccer goalie making a save\",\n",
    "\"A bear in a trench coat as a private investigator\",\n",
    "\"A bear as a judge in a courtroom\",\n",
    "\"A bear wearing a toga as a philosopher\",\n",
    "\"A bear as a gardener tending to a rose garden\",\n",
    "\"A bear as a ship captain steering a boat\",\n",
    "]\n",
    "run_grid(prompts, 5, pipe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90007e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if pipe1:\n",
    "    del pipe1\n",
    "if pipe2:\n",
    "    del pipe2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07178c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e65cf207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_tensor_memory(namespace=globals()):\n",
    "    print(\"GPU Tensors in current namespace:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_memory = 0\n",
    "    for name, obj in namespace.items():\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            size_mb = obj.numel() * obj.element_size() / 1024 / 1024\n",
    "            total_memory += size_mb\n",
    "            print(f\"{name:20s} | Shape: {str(obj.shape):20s} | \"\n",
    "                  f\"Size: {size_mb:8.2f} MB | Device: {obj.device}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Total memory used: {total_memory:.2f} MB\")\n",
    "\n",
    "# Usage\n",
    "check_tensor_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46759ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, obj in list(globals().items()): # Or locals() if in a function\n",
    "    if torch.is_tensor(obj) and obj.is_cuda:\n",
    "        print(f\"Variable: {name}, Shape: {obj.shape}, Data type: {obj.dtype}, Memory (MB): {(obj.nelement() * obj.element_size()) / 1024**2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1340d4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "def show_gpu_tensors():\n",
    "    # Get IPython instance to access user namespace\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ipython = get_ipython()\n",
    "        user_ns = ipython.user_ns if ipython else globals()\n",
    "    except:\n",
    "        user_ns = globals()\n",
    "    \n",
    "    print(\"GPU Tensors in IPython namespace:\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    gpu_tensors = []\n",
    "    \n",
    "    # Check all variables in user namespace\n",
    "    for var_name, obj in user_ns.items():\n",
    "        if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "            size_mb = obj.numel() * obj.element_size() / 1024 / 1024\n",
    "            gpu_tensors.append({\n",
    "                'name': var_name,\n",
    "                'tensor': obj,\n",
    "                'size_mb': size_mb\n",
    "            })\n",
    "        # Also check if it's a list/dict containing tensors\n",
    "        elif isinstance(obj, (list, tuple)):\n",
    "            for i, item in enumerate(obj):\n",
    "                if isinstance(item, torch.Tensor) and item.is_cuda:\n",
    "                    size_mb = item.numel() * item.element_size() / 1024 / 1024\n",
    "                    gpu_tensors.append({\n",
    "                        'name': f\"{var_name}[{i}]\",\n",
    "                        'tensor': item,\n",
    "                        'size_mb': size_mb\n",
    "                    })\n",
    "        elif isinstance(obj, dict):\n",
    "            for key, item in obj.items():\n",
    "                if isinstance(item, torch.Tensor) and item.is_cuda:\n",
    "                    size_mb = item.numel() * item.element_size() / 1024 / 1024\n",
    "                    gpu_tensors.append({\n",
    "                        'name': f\"{var_name}['{key}']\",\n",
    "                        'tensor': item,\n",
    "                        'size_mb': size_mb\n",
    "                    })\n",
    "    \n",
    "    # Sort by memory usage\n",
    "    gpu_tensors.sort(key=lambda x: x['size_mb'], reverse=True)\n",
    "    \n",
    "    total_memory = sum(t['size_mb'] for t in gpu_tensors)\n",
    "    \n",
    "    for tensor_info in gpu_tensors:\n",
    "        tensor = tensor_info['tensor']\n",
    "        print(f\"Variable: {tensor_info['name']:25s} | \"\n",
    "              f\"Shape: {str(tuple(tensor.shape)):20s} | \"\n",
    "              f\"Size: {tensor_info['size_mb']:8.2f} MB | \"\n",
    "              f\"Type: {str(tensor.dtype):15s} | \"\n",
    "              f\"Device: {tensor.device}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total GPU tensors found: {len(gpu_tensors)}\")\n",
    "    print(f\"Total memory used: {total_memory:.2f} MB ({total_memory/1024:.2f} GB)\")\n",
    "    \n",
    "    # Show overall GPU memory status\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory - Allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB, \"\n",
    "              f\"Reserved: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "# Run it\n",
    "show_gpu_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd739cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_gpu_tensors():\n",
    "    print(\"All GPU tensors in memory (including unreferenced):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gpu_tensors = []\n",
    "    \n",
    "    # Use garbage collector to find all tensor objects\n",
    "    for obj in gc.get_objects():\n",
    "        if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "            size_mb = obj.numel() * obj.element_size() / 1024 / 1024\n",
    "            gpu_tensors.append({\n",
    "                'tensor': obj,\n",
    "                'size_mb': size_mb,\n",
    "                'id': id(obj)\n",
    "            })\n",
    "    \n",
    "    # Sort by size\n",
    "    gpu_tensors.sort(key=lambda x: x['size_mb'], reverse=True)\n",
    "    \n",
    "    total_memory = sum(t['size_mb'] for t in gpu_tensors)\n",
    "    \n",
    "    print(f\"{'Index':<6} {'Shape':<20} {'Size (MB)':<12} {'Type':<15} {'ID':<15}\")\n",
    "    print(\"-\" * 75)\n",
    "    \n",
    "    for i, tensor_info in enumerate(gpu_tensors):\n",
    "        tensor = tensor_info['tensor']\n",
    "        print(f\"{i+1:<6} {str(tuple(tensor.shape)):<20} \"\n",
    "              f\"{tensor_info['size_mb']:<12.2f} {str(tensor.dtype):<15} \"\n",
    "              f\"{tensor_info['id']:<15}\")\n",
    "    \n",
    "    print(\"-\" * 75)\n",
    "    print(f\"Total tensors: {len(gpu_tensors)}, Total memory: {total_memory:.2f} MB\")\n",
    "\n",
    "find_all_gpu_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2fafe420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "from IPython import get_ipython\n",
    "\n",
    "def find_tensor_references_safe():\n",
    "    # Get the IPython namespace\n",
    "    ipython = get_ipython()\n",
    "    user_ns = ipython.user_ns if ipython else globals()\n",
    "    \n",
    "    # Target the huge tensors by their IDs\n",
    "    huge_tensor_ids = [127537981066192, 127537981072672, 127537981065952]\n",
    "    \n",
    "    print(\"Searching for references to huge tensors...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create a snapshot of the namespace to avoid iteration issues\n",
    "    namespace_snapshot = dict(user_ns)\n",
    "    \n",
    "    # Function to recursively search through objects\n",
    "    def search_object(obj, path=\"\", max_depth=2, current_depth=0):\n",
    "        if current_depth > max_depth:\n",
    "            return []\n",
    "        \n",
    "        references = []\n",
    "        \n",
    "        try:\n",
    "            if isinstance(obj, torch.Tensor) and obj.is_cuda:\n",
    "                if id(obj) in huge_tensor_ids:\n",
    "                    size_mb = obj.numel() * obj.element_size() / 1024 / 1024\n",
    "                    references.append((path, obj.shape, size_mb, id(obj)))\n",
    "            \n",
    "            elif isinstance(obj, (list, tuple)) and current_depth < max_depth and len(obj) < 1000:\n",
    "                for i, item in enumerate(obj[:100]):  # Limit to first 100 items\n",
    "                    new_path = f\"{path}[{i}]\" if path else f\"item[{i}]\"\n",
    "                    references.extend(search_object(item, new_path, max_depth, current_depth + 1))\n",
    "            \n",
    "            elif isinstance(obj, dict) and current_depth < max_depth and len(obj) < 1000:\n",
    "                for key, item in list(obj.items())[:100]:  # Limit and convert to list\n",
    "                    new_path = f\"{path}['{key}']\" if path else f\"dict['{key}']\"\n",
    "                    references.extend(search_object(item, new_path, max_depth, current_depth + 1))\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Skip problematic objects\n",
    "            pass\n",
    "        \n",
    "        return references\n",
    "    \n",
    "    # Search through snapshot of variables\n",
    "    all_references = []\n",
    "    for var_name, obj in namespace_snapshot.items():\n",
    "        if not var_name.startswith('_') and var_name not in ['In', 'Out']:  # Skip IPython internals\n",
    "            try:\n",
    "                references = search_object(obj, var_name)\n",
    "                all_references.extend(references)\n",
    "            except Exception as e:\n",
    "                print(f\"Skipped {var_name} due to error: {type(e).__name__}\")\n",
    "    \n",
    "    # Display results\n",
    "    if all_references:\n",
    "        print(\"Found references to huge tensors:\")\n",
    "        for path, shape, size_mb, tensor_id in all_references:\n",
    "            print(f\"Variable: {path}\")\n",
    "            print(f\"  Shape: {shape}\")\n",
    "            print(f\"  Size: {size_mb:.2f} MB\")\n",
    "            print(f\"  ID: {tensor_id}\")\n",
    "            print(\"-\" * 40)\n",
    "    else:\n",
    "        print(\"No direct references found in user namespace.\")\n",
    "\n",
    "find_tensor_references_safe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "470a89fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_autograd_tensors():\n",
    "    print(\"Checking for autograd/gradient tensors...\")\n",
    "    \n",
    "    # Find all tensors that require gradients\n",
    "    all_tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]\n",
    "    grad_tensors = [t for t in all_tensors if t.requires_grad and t.is_cuda]\n",
    "    \n",
    "    large_grad_tensors = []\n",
    "    for tensor in grad_tensors:\n",
    "        size_mb = tensor.numel() * tensor.element_size() / 1024 / 1024\n",
    "        if size_mb > 100:  # Only show large ones\n",
    "            large_grad_tensors.append((tensor, size_mb))\n",
    "    \n",
    "    large_grad_tensors.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for tensor, size_mb in large_grad_tensors:\n",
    "        print(f\"Gradient tensor: Shape {tensor.shape}, Size: {size_mb:.2f} MB\")\n",
    "        print(f\"  requires_grad: {tensor.requires_grad}\")\n",
    "        print(f\"  grad_fn: {tensor.grad_fn}\")\n",
    "        print(f\"  ID: {id(tensor)}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "check_autograd_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3318951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_computation_graph_tensors():\n",
    "    print(\"Looking for tensors in computation graphs...\")\n",
    "    \n",
    "    target_ids = {127537981066192, 127537981072672, 127537981065952}\n",
    "    \n",
    "    # Get all tensors\n",
    "    all_tensors = [obj for obj in gc.get_objects() if isinstance(obj, torch.Tensor)]\n",
    "    \n",
    "    # Find tensors with grad_fn (part of computation graph)\n",
    "    graph_tensors = []\n",
    "    for tensor in all_tensors:\n",
    "        if tensor.is_cuda and tensor.grad_fn is not None:\n",
    "            size_mb = tensor.numel() * tensor.element_size() / 1024 / 1024\n",
    "            if id(tensor) in target_ids or size_mb > 100:  # Large tensors or our targets\n",
    "                graph_tensors.append((tensor, size_mb))\n",
    "    \n",
    "    graph_tensors.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for tensor, size_mb in graph_tensors:\n",
    "        print(f\"Graph tensor: Shape {tensor.shape}, Size: {size_mb:.2f} MB\")\n",
    "        print(f\"  grad_fn: {type(tensor.grad_fn).__name__ if tensor.grad_fn else None}\")\n",
    "        print(f\"  ID: {id(tensor)} {'<-- TARGET' if id(tensor) in target_ids else ''}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "find_computation_graph_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8d8284",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"Tensors on GPU:\")\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) and obj.is_cuda:\n",
    "            print(f\"  Size: {obj.size()}, Dtype: {obj.dtype}, Device: {obj.device}, Shape: {obj.shape}\")\n",
    "            # You can add more properties if needed, like obj.numel() for total elements\n",
    "    except: # Handles cases where obj is not a tensor or other errors, or if it's a tensor that doesn't have a .is_cuda attribute (older PyTorch or different tensor types)\n",
    "        pass\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"\\nCUDA Memory Summary:\")\n",
    "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "else:\n",
    "    print(\"\\nCUDA is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c37a31e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Virtual Environment",
   "language": "python",
   "name": "my_venv_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
